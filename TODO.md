so i need to  take steps towards streamlining and eventual automation of generating and refining initial prompts to be fed into the script factory

i also need to adjust the script factory to include the time it takes to generate each script because some scripts are taking a long time to generate and i need to be able to compare them for efficiency in terms of pytests and how solid the code is compared to how long it takes to generate.

also track the length of the prompts compared to the number of code blocks in the output scripts.

we may also track the number of lines of code in the output scripts compared to the number of lines of code in the prompts.

we should consider an additional final pass that runs after the pytests and takes that and other data and assesses the quality of the script including factors like how complex the prompt was, how small the LLM is, how long it took to generate, how many pytests passed, how many failed, and how many were skipped.  

once we have an architecture in place for assessing automated scripts we can use this data to refine the prompts to be fed into the script factory. then once we refine prompts and improve the scripts we can run the pytests again and use the data to refine the prompts again and so on until we have a set of prompts that generate scripts that pass the pytests most of the time. (at least in theory) we can also run into issues though where it just automates and removes creativity so the tests work but don't properly account for edge cases or the parent scripts aren't thorough enough to begin with so the test design is too simplified for a real world application.



consider implementing  a way of analyzing the scripts generated by the script factory and comparing them to other benchmarks for the input prompts so i'll need other analytical passes on the original prompts of the script factory like 'sentimental value' or code efficiency (functionality per line of code) and then i can use this data to refine the prompts to be fed into the script factory.

we need the functions and classes to be imported from the parent scripts into the test scripts so i can run the tests on them without errors. we may consider whether to use a relative import or an absolute import

the script factory should be able to generate a report of the scripts it has generated and the time it took to generate them and the number of pytests that passed and failed for each script

we need to be able to run the tests on the scripts generated by the script factory in a way as to continue onto the next test if one script fails to run its tests (not to be confused with failed tests: i mean we need to be able to continue onto the next test if one script fails to run its tests at all )

the idea is to eventually have a metric of how many of the scripts in each batch successfully ran tests, and then out of the scripts that successfully ran tests, how many passed and how many failed, and then use this data to refine the prompts to be fed into the script factory to generate more error resistant scripts

one error i keep getting is module not found error and it's because the import lines are wrong. we could change the directory  structure to be uniform across each batch and only have the information about the model used to generate the script in the metadata instead of the filename and then just have each script in the batches be named like BATCH1_PROMPT(1-25).py. since they are still technically all uniquely identified by batch and prompt number per folder we should be able to do absolute imports. I'm not exactly sure so we'll need to test this out

so first lets copy the scripts from each batch into a new folder with the same subfolders (the subfolders will be for each model configuration) for processing and rename them BATCH1_PROMPT(1-25).py

my inclination is the Compilationscripts folder would be good but we could also make a different folder like "Preprocessing" where we just rename the scripts and copy them to the new folders. the new folders should have the same structure as the all_runs folder or alternatively be organized by model configuration i think having both would be fine with only 1000 small scripts.

so we need the default all_runs file structure as "process by batch" and then another file structure that is "process by model" so there will be a preprocessing folder with all  the scripts in both the batch and model subfolders but organized accordingly the filenames of the scripts should be consistent for both situations.

then we can generate processing scripts that make sure the scripts are in the proper format and only contain python code without extra non-code from the LLM and include comment for the LLM to use to generate proper import statements when making the testing scripts

we can then run that on a batch and see if either script processes the scripts successfully especially regarding imports.

so to recap the most urgent thing is we need to isolate the issue of imports and make a standalone script that can import the functions and classes from the parent scripts and inject that into the testing scripts once they are generated. it needs self correcting measures (be it another pass through an llm or some other method directly solved through python) that checks for typos and other issues with the import statements and corrects them (like if the llm making the test script tries importing a function or class and then i use the standalone script and it has redundant or worse error generating imports)

for now, the script needs to be able to run on a batch of scripts with one run per batch and eventually be able to run on new batches as they are generated but that's a while out.

if i can get it to run on one batch it'll be good to streamline it to run on multiple batches at once, which would likely be easier if the folder structure is organized by model configuration instead of batch number.

still we should test both folder structures to see which is better for processing.


i'm not sure if these instructions are clear enough 

it also may be better to first prepare a batch of test scripts with whatever import errors occur in them to test the import correction script. 

Although it is urgent to prepare an import correction script, i cannot effectively test it without a batch of test scripts with import errors generated in the correct preprocessing folders.

so right now i need a batch of test scripts with import errors generated in the correct preprocessing folders to test the import correction script. to get that we need to refine the @testpromptgenerator.py or start fresh with a new script the script generator doesn't need any functions to trim the output of the LLM but it should include timestamps and information for how long it took to generate the test file. the test file should be TEST_BATCH#_PROMPT# and put in a "processing" folder (two separate copies in the two different file structures outlined previously )

so the preprocessing folder just renames the files and the processing folder is where the TEST files are initially put after generation to be processed by the import correction script, and then they will be moved to the "Tests" folder

so for now we need to ensure the testpromptgenerator generates to the correct processing folder and before that we need to ensure that the preprocessing folder contains the files correctly renamed from the original all_runs folder 

so i need a script for preprocessing and a script for processing. the preprocessing script will take the folder structure of the all_runs folder and rename the files to BATCH#_PROMPT#.py and copy them to the preprocessing folder
the processing script will run the testpromptgenerator.py script and generate the test files to the tests folder

